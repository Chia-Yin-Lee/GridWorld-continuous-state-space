{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Chia-Yin-Lee/GridWorld-continuous-state-space/blob/main/A2_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xMPenvVExk3h",
        "outputId": "96006f01-3c67-4705-c498-f28fb9981221"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/MyDrive/SMU_MITB_RL\n"
          ]
        }
      ],
      "source": [
        "#@title connect google drive folder\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "%cd /content/drive/MyDrive/SMU_MITB_RL"
      ],
      "id": "xMPenvVExk3h"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "312ce7b4"
      },
      "outputs": [],
      "source": [
        "#@title import packages\n",
        "\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from copy import deepcopy\n",
        "import math\n",
        "import time\n",
        "import os\n",
        "import random\n",
        "import imageio\n",
        "\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.layers import Input, Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "from collections import deque\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "import matplotlib.colors as mcolors\n",
        "from matplotlib.cm import ScalarMappable\n",
        "from PIL import Image, ImageChops"
      ],
      "id": "312ce7b4"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "73cd81ed"
      },
      "source": [
        "### Question 1"
      ],
      "id": "73cd81ed"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3433ed93"
      },
      "source": [
        "### Q1(a) Do you like my code?\n",
        "Yes\n",
        "### Q1(b) Modify the given environment according to this problem."
      ],
      "id": "3433ed93"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "76F_tgiQ_jr4"
      },
      "source": [
        "Suppose we want to modify the environment to deal with a gridworld that is no longer formed\n",
        "by grids. Specifically, it has the following aspects:\n",
        "- The 'world' has a continuous space, spanning from coordinates 0 to 10 along both the horizontal and vertical dimensions.\n",
        "- The agent moves a distance of exactly 1 unit each step, which can be in any direction in increments of 10 degrees. Therefore, there is a total of 36 possible actions, unlike previously where only four directions (0/90/180/270 degrees) had been considered.\n",
        "- There is no wind or cliff. The agent cannot go outside of the gridworld; clip the position to deal with this.\n",
        "- Movement is deterministic and there is no slip.\n",
        "- The agent starts at position [2, 2].\n",
        "- Anywhere within the region [7, 7] to [8, 8] is the goal, for which the agent receives a reward of +100 and the episode terminates. For example, landing in positions [7.2, 7.7] or [7.9, 7.3] constitute to landing in the goal.\n",
        "- Anywhere within the region [2, 4] to [8, 5] is the trap, for which the agent receives a reward of -50. If the agent keeps walking within these 6 square units of trap region, it receives the negative reward each time, regardless of whether the steps are successive.\n",
        "- All moves which does not end in the goal or trap region will have a reward of -1.\n",
        "\n",
        "Modify the given environment according to this problem. In addition, modify the agent such that Q is estimated from a functional approximator with three hidden layers, with an appropriate number of nodes in each layer. **The structure of `SlipperyWindyCliffGridWorld`, eg. the names of each method and each of the inputs and outputs, must remain the same.** You may create the functional approximator as a class of its own, expand an existing class (with new methods if necessary), or in any way you deem fit."
      ],
      "id": "76F_tgiQ_jr4"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "7da5cee1"
      },
      "outputs": [],
      "source": [
        "#@title Modified environment\n",
        "\n",
        "class World:\n",
        "    def __init__(self):\n",
        "\n",
        "        self.max_row = 10\n",
        "        self.max_col = 10\n",
        "        self.initial_pos = [2, 2]\n",
        "\n",
        "        # no wind, no cliff, no slip\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        # Use exploring starts by default\n",
        "        self.is_done = False\n",
        "        self.cur_state = deepcopy(self.initial_pos)\n",
        "        return self.cur_state\n",
        "\n",
        "    def _get_state_dim(self):\n",
        "        # return np.array(self.grid).shape\n",
        "        return np.array([10])\n",
        "\n",
        "    def _get_action_dim(self):\n",
        "        return np.array([36])  # 36 actions\n",
        "\n",
        "    def _get_next_state(self, x, y, action_idx, distance=1):\n",
        "        angle_radians = math.radians(action_idx * 10)\n",
        "\n",
        "        delta_x = distance * math.cos(angle_radians)\n",
        "        delta_y = distance * math.sin(angle_radians)\n",
        "\n",
        "        new_x = x + delta_x\n",
        "        new_y = y + delta_y\n",
        "        return new_x, new_y\n",
        "\n",
        "    def transition(self, state, action):\n",
        "        if self.is_done:\n",
        "            return 0, state, True\n",
        "\n",
        "        next_state = deepcopy(state)\n",
        "\n",
        "        action = np.random.choice(36)  # 36 actions\n",
        "\n",
        "        # action idx -> next state\n",
        "        next_state = self._get_next_state(x = state[0], y = state[1], action_idx = action)\n",
        "\n",
        "        next_state = np.clip(next_state, [0, 0], [self.max_row, self.max_col]).tolist()\n",
        "\n",
        "        row, col = next_state\n",
        "\n",
        "        if row >= 2 and row <= 8 and col >= 4 and col <= 5:\n",
        "            reward = -50  # trap\n",
        "            self.is_done = False\n",
        "        elif row >= 7 and row <= 8 and col >= 7 and col <= 8:\n",
        "            reward = 100   # goal\n",
        "            self.is_done = True\n",
        "        else:\n",
        "            reward = -1\n",
        "            self.is_done = False\n",
        "\n",
        "        return reward, next_state, self.is_done"
      ],
      "id": "7da5cee1"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "81e2038c",
        "outputId": "108f4f3c-5af1-4179-f7e3-5cdf4e0984a2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[([2, 2], 31, -1), ([1.0603073792140916, 1.6579798566743313], 17, -1), ([1.9263327829985304, 2.157979856674331], 10, -1), ([2.6923772261175083, 2.8007674663608704], 25, -1), ([3.3351648358040475, 2.0347230232418925], 11, -1), ([3.977952445490587, 1.2686785801229143], 13, -1), ([4.843977849275025, 0.7686785801229139], 30, -1), ([3.977952445490587, 0.2686785801229138], 25, -1), ([3.111927041706148, 0.7686785801229137], 16, -1), ([4.051619662492056, 1.1106987234485823], 9, -1), ([4.2252678401589865, 0.1258909704363742], 29, -1), ([3.4592233970400086, 0.0], 35, -1), ([2.8164357873534693, 0.766044443118978], 26, -1), ([3.5824802304724472, 1.4088320528055172], 9, -1), ([3.4088320528055167, 2.3936398058177253], 35, -1), ([2.642787609686539, 3.0364274155042645], 24, -1), ([3.642787609686539, 3.0364274155042645], 17, -1), ([3.816435787353469, 2.0516196624920564], 4, -1), ([3.4744156440278, 2.991312283277965], 11, -1), ([3.9744156440278, 3.8573376870624037], 20, -50), ([4.914108264813708, 4.199357830388072], 22, -50), ([3.9293005118015003, 4.025709652721142], 11, -50), ([3.5872803684758314, 4.965402273507051], 31, -50), ([2.6024726154636233, 4.791754095840121], 19, -50), ([3.4684980192480617, 4.291754095840121], 10, -50), ([2.4684980192480617, 4.291754095840121], 17, -50), ([3.3345234230325005, 4.791754095840121], 34, -50), ([2.3497156700202924, 4.965402273507051], 30, -1), ([1.410023049234384, 5.30742241683272], 13, -1), ([0.9100230492343842, 6.173447820617159], 30, -1), ([1.7760484530188227, 6.673447820617159], 21, -1), ([1.7760484530188227, 7.673447820617159], 29, -1), ([0.7912407000066147, 7.4997996429502285], 30, -1), ([1.1332608433322837, 6.5601070221643205], 4, -1), ([1.3069090209992136, 5.575299269152112], 29, -1), ([0.5408645778802357, 6.218086878838652], 14, -1), ([0.0, 5.452042435719674], 7, -1), ([0.0, 4.586017031935236], 10, -1), ([0.9396926207859084, 4.928037175260904], 35, -1), ([1.8057180245703472, 5.428037175260904], 5, -50), ([2.6717434283547856, 4.928037175260904], 31, -1), ([2.4980952506878555, 5.912844928273112], 25, -50), ([2.6717434283547856, 4.928037175260904], 13, -1), ([2.845391606021716, 5.912844928273112], 15, -50), ([3.187411749347385, 4.973152307487204], 6, -50), ([2.8453916060217166, 4.0334596867012955], 35, -1), ([3.711417009806155, 3.533459686701295], 7, -50), ([3.711417009806155, 4.5334596867012955], 9, -1), ([3.885065187473085, 3.5486519336890874], 32, -50), ([3.385065187473085, 4.414677337473526], 12, -1), ([4.027852797159625, 5.180721780592504], 22, -1), ([4.369872940485293, 6.120414401378412], 17, -1), ([5.012660550171833, 6.8864588444973895], 30, -1), ([5.512660550171833, 7.752484248281828], 30, -1), ([5.512660550171833, 6.752484248281828], 0, -1), ([6.278704993290811, 6.109696638595288], 27, -1), ([5.512660550171833, 6.752484248281828], 3, -1), ([5.339012372504903, 5.76767649526962], 33, -1), ([6.278704993290811, 6.109696638595288], 33, -1), ([6.620725136616479, 7.049389259381196], 22, -1), ([7.560417757402387, 6.707369116055528], 6, -1), ([7.560417757402387, 5.707369116055528], 22, -1), ([6.794373314283409, 6.3501567257420675], 21, -1), ([7.660398718067848, 5.8501567257420675], 21, -1), ([8.526424121852287, 6.3501567257420675], 13, -1), ([8.352775944185357, 5.365348972729859], 5, -1), ([7.367968191173149, 5.191700795062929], 30, -1), ([8.352775944185357, 5.018052617395999], 33, -1), ([8.526424121852287, 4.033244864383791], 32, -1), ([7.541616368840079, 3.8595966867168605], 20, -50), ([7.883636512165747, 4.799289307502769], 5, -1), ([8.649680955284726, 5.442076917189308], 17, -1), ([9.589373576070635, 5.100056773863639], 6, -1), ([10.0, 5.742844383550178], 0, -1), ([10.0, 5.569196205883248], 17, -1), ([9.233955556881021, 6.211983815569788], 25, -1), ([9.407603734547951, 5.22717606255758], 6, -1), ([10.0, 5.9932205056765575], 14, -1), ([9.82635182233307, 5.008412752664349], 31, -1), ([10.0, 4.36562514297781], 35, -1), ([10.0, 3.72283753329127], 1, -1), ([10.0, 2.8568121295068316], 30, -1), ([9.015192246987793, 3.0304603071737617], 23, -1), ([8.372404637301253, 2.264415864054784], 13, -1), ([9.238430041085692, 1.7644158640547833], 17, -1), ([9.738430041085692, 2.630441267839222], 19, -1), ([10.0, 2.1304412678392217], 6, -1), ([10.0, 1.1456335148270136], 20, -1), ([9.015192246987793, 0.9719853371600831], 0, -1), ([9.015192246987793, 0.0], 4, -1), ([10.0, 0.0], 7, -1), ([9.13397459621556, 0.0], 29, -1), ([9.47599473954123, 0.9396926207859083], 34, -1), ([8.47599473954123, 0.9396926207859084], 34, -1), ([8.8180148828669, 1.1102230246251565e-16], 32, -1), ([9.8180148828669, 1.1102230246251565e-16], 12, -1), ([9.47599473954123, 0.9396926207859085], 11, -1), ([10.0, 0.7660444431189781], 27, -1), ([9.5, 1.6320698469034167], 12, -1), ([8.560307379214091, 1.290049703577748], 4, -1), ([9.560307379214091, 1.290049703577748], 2, -1), ([8.575499626201884, 1.1164015259108175], 9, -1), ([7.7094742224174455, 1.6164015259108175], 27, -1), ([7.535826044750515, 2.6012092789230254], 26, -1), ([7.193805901424847, 3.540901899708934], 18, -50), ([7.959850344543825, 4.183689509395473], 9, -1), ([7.459850344543824, 3.3176641056110343], 1, -1), ([6.817062734857284, 2.5516196624920564], 28, -1), ([5.832254981845076, 2.377971484825126], 29, -1), ([5.332254981845075, 1.5119460810406875], 8, -1), ([6.271947602630984, 1.1699259377150188], 4, -1), ([6.098299424964054, 0.18511818470281083], 8, -1), ([6.741087034650594, 0.9511626278217888], 32, -1), ([5.741087034650594, 0.951162627821789], 28, -1), ([6.680779655436503, 1.2931827711474577], 11, -1), ([7.665587408448711, 1.1195345934805274], 23, -1), ([7.16558740844871, 0.25350918969608904], 21, -1), ([7.33923558611564, 0.0], 23, -1), ([6.9972154427899715, 0.9396926207859084], 2, -1), ([6.131190039005533, 1.4396926207859084], 28, -1), ([6.631190039005533, 0.5736672170014698], 35, -1), ([7.397234482124511, 0.0], 28, -1), ([8.163278925243489, 0.6427876096865393], 22, -1), ([7.663278925243489, 0.0], 8, -1), ([6.678471172231281, 0.17364817766693028], 35, -1), ([6.678471172231281, 1.1736481776669303], 8, -1), ([5.8124457684468425, 0.6736481776669302], 0, -1), ([4.946420364662404, 1.17364817766693], 14, -1), ([3.961612611650196, 1.3472963553338604], 25, -1), ([4.303632754975864, 2.2869889761197686], 20, -1), ([3.6608451452893247, 1.5209445330007907], 17, -1), ([4.645652898301533, 1.694592710667721], 17, -1), ([4.645652898301533, 2.694592710667721], 29, -1), ([5.511678302085971, 3.194592710667721], 20, -1), ([4.745633858966993, 3.837380320354261], 2, -50), ([3.8796084551825545, 4.337380320354261], 17, -1), ([3.2368208454960152, 3.571335877235283], 19, -1), ([3.2368208454960152, 2.571335877235283], 27, -1), ([2.370795441711577, 2.071335877235283], 13, -1), ([1.3859876886993687, 1.8976876995683525], 32, -1), ([0.7432000790128294, 2.6637321426873304], 1, -1), ([0.7432000790128295, 3.6637321426873304], 14, -1), ([0.4011799356871609, 2.724039521901422], 8, -1), ([1.2672053394715994, 2.2240395219014215], 26, -1), ([1.9099929491581387, 1.4579950787824434], 32, -1), ([1.4099929491581389, 2.324020482566882], 19, -1), ([1.2363447714912086, 1.3392127295546739], 10, -1), ([0.23634477149120858, 1.339212729554674], 14, -1), ([0.0, 0.9971925862290054], 11, -1), ([0.0, 0.8235444085620749], 29, -1), ([0.5000000000000001, 0.0], 21, -1), ([0.0, 0.3420201433256689], 20, -1), ([0.0, 1.326827896337877], 12, -1), ([0.984807753012208, 1.1531797186709467], 8, -1), ([1.750852196131186, 1.795967328357486], 0, -1), ([2.750852196131186, 1.795967328357486], 34, -1), ([2.408832052805517, 2.735659949143394], 28, -1), ([3.0516196624920564, 3.501704392262372], 11, -50), ([2.408832052805517, 4.26774883538135], 12, -1), ([3.3485246735914256, 3.9257286920556815], 3, -1), ([3.6905448169170945, 2.986036071269773], 13, -1), ([4.690544816917095, 2.986036071269773], 35, -1), ([4.864192994584025, 3.9708438242819812], 25, -1), ([4.098148551465047, 3.328056214595442], 23, -1), ([3.113340798452839, 3.501704392262372], 5, -1), ([3.879385241571817, 2.8589167825758324], 14, -1), ([4.379385241571817, 3.7249421863602707], 4, -1), ([5.319077862357725, 3.3829220430346023], 14, -1), ([5.9618654720442645, 2.6168775999156244], 15, -1), ([5.319077862357725, 3.3829220430346023], 25, -1), ([5.661098005683394, 2.443229422248694], 18, -1), ([6.003118149009063, 3.3829220430346023], 1, -1), ([6.176766326675993, 2.398114290022394], 12, -1), ([6.003118149009063, 1.413306537010186], 35, -1), ([6.7691625921280405, 2.0560941466967253], 30, -1), ([6.2691625921280405, 2.922119550481164], 13, -1), ([6.61118273545371, 1.9824269296952557], 33, -1), ([6.61118273545371, 0.9824269296952557], 32, -1), ([5.671490114667802, 1.3244470730209246], 1, -1), ([5.671490114667802, 2.3244470730209246], 13, -1), ([6.53751551845224, 2.8244470730209246], 20, -1), ([7.03751551845224, 1.958421669236486], 21, -1), ([6.86386734078531, 2.943229422248694], 3, -1), ([7.629911783904288, 2.300441812562154], 32, -1), ([8.395956227023266, 1.6576542028756145], 8, -1), ([8.053936083697597, 0.7179615820897061], 21, -1), ([7.0691283306853885, 0.8916097597566364], 17, -1), ([8.008820951471296, 1.233629903082305], 24, -1), ([7.5088209514712965, 2.099655306866744], 3, -1), ([8.448513572257205, 2.4416754501924127], 33, -1), ([7.682469129138227, 3.084463059878952], 5, -50), ([7.508820951471297, 4.06927081289116], 8, -1), ([8.448513572257205, 3.7272506695654917], 12, -50), ([7.582488168472767, 4.227250669565492], 32, -50), ([6.816443725353789, 4.870038279252031], 33, -50), ([6.050399282234811, 4.227250669565493], 3, -50), ([6.9164246860192495, 4.727250669565493], 17, -1), ([6.050399282234811, 5.227250669565493], 32, -1), ([5.110706661448903, 5.569270812891162], 19, -1), ([6.050399282234812, 5.91129095621683], 21, -1), ([5.050399282234812, 5.91129095621683], 29, -50), ([4.708379138909144, 4.971598335430922], 3, -50), ([5.693186891921352, 4.797950157763992], 30, -1), ([6.035207035247021, 3.858257536978084], 31, -1), ([6.901232439031459, 3.3582575369780834], 19, -1), ([7.886040192043668, 3.531905714645014], 32, -50), ([7.7123920143767375, 4.516713467657222], 30, -1), ([7.886040192043668, 3.5319057146450135], 0, -1), ([8.870847945055875, 3.705553892311944], 19, -1), ([8.528827801730205, 2.7658612715260356], 20, -1), ([9.528827801730205, 2.7658612715260356], 32, -1), ([9.028827801730205, 1.8998358677415972], 22, -1), ([8.044020048717998, 1.7261876900746667], 18, -1), ([7.870371871051068, 2.710995443086875], 15, -1), ([8.044020048717998, 3.695803196099083], 22, -1), ([8.544020048717998, 4.561828599883522], 15, -1), ([8.370371871051068, 5.54663635289573], 26, -1), ([9.136416314170045, 4.90384874320919], 25, -1), ([8.794396170844376, 3.9641561224232817], 34, -1), ([9.779203923856583, 4.137804300090212], 10, -1), ([10.0, 4.90384874320919], 26, -1), ([9.82635182233307, 5.888656496221398], 17, -1), ([10.0, 5.388656496221397], 17, -1), ([9.82635182233307, 6.3734642492336055], 17, -1), ([10.0, 7.3131568700195135], 23, -1), ([10.0, 6.3131568700195135], 2, -1), ([10.0, 6.3131568700195135], 33, -1), ([10.0, 7.297964623031722], 16, -1), ([10.0, 7.63998476635739], 27, -1), ([9.5, 8.50601017014183], 26, -1), ([9.0, 7.639984766357391], 7, -1), ([8.82635182233307, 6.655177013345183], 10, -1), ([9.0, 5.670369260332975], 21, -1), ([8.015192246987793, 5.844017437999905], 21, -1), ([8.78123669010677, 5.201229828313365], 7, -1), ([7.78123669010677, 5.201229828313365], 14, -1), ([6.915211286322331, 5.701229828313365], 22, -1), ([7.9000190393345395, 5.527581650646435], 8, -1), ([8.666063482453517, 4.884794040959895], 33, -1), ([7.800038078669078, 5.384794040959895], 8, -1), ([8.442825688355617, 4.618749597840917], 7, -50), ([7.503133067569709, 4.960769741166587], 15, -50), ([6.860345457883169, 4.194725298047609], 13, -50), ([5.860345457883169, 4.194725298047609], 6, -1), ([6.800038078669077, 3.8527051547219404], 9, -1), ([5.800038078669077, 3.8527051547219404], 11, -1), ([5.033993635550099, 3.209917545035401], 0, -1), ([4.167968231765661, 2.709917545035401], 19, -1), ([4.341616409432591, 3.6947252980476093], 12, -1), ([4.841616409432591, 2.8286998942631705], 3, -1), ([5.826424162444799, 3.002348071930101], 31, -1), ([5.326424162444799, 3.86837347571454], 4, -1), ([5.826424162444799, 3.0023480719301014], 4, -1), ([4.841616409432591, 3.1759962495970315], 3, -1), ([3.9019237886466827, 3.5180163929227004], 24, -50), ([4.2439439319723515, 4.457709013708609], 7, -1), ([3.4778994888533736, 3.8149214040220696], 35, -50), ([2.611874085068935, 4.314921404022069], 8, -1), ([3.477899488853373, 3.8149214040220687], 17, -50), ([2.5382068680674648, 4.156941547347738], 22, -50), ([3.477899488853373, 4.498961690673406], 22, -1), ([3.6515476665203033, 3.514153937661198], 29, -1), ([2.711855045734395, 3.8561740809868668], 25, -1), ([3.711855045734395, 3.8561740809868668], 20, -1), ([3.5382068680674643, 2.8713663279746586], 7, -1), ([4.477899488853373, 2.52934618464899], 13, -1), ([5.343924892637811, 3.02934618464899], 13, -1), ([6.283617513423719, 3.371366327974659], 6, -1), ([6.926405123110259, 2.605321884855681], 18, -1), ([7.866097743896168, 2.2633017415300127], 26, -1), ([8.208117887221837, 1.3236091207441043], 5, -1), ([7.223310134209629, 1.1499609430771738], 10, -1), ([7.989354577328607, 1.792748552763713], 33, -1), ([7.004546824316399, 1.9663967304306433], 5, -1), ([7.004546824316399, 2.9663967304306436], 12, -1), ([7.944239445102307, 3.3084168737563124], 24, -1), ([8.587027054788846, 2.542372430637334], 28, -1), ([8.245006911463177, 3.4820650514232425], 4, -1), ([8.071358733796247, 4.46687280443545], 33, -1), ([7.305314290677269, 3.824085194748911], 9, -1), ([7.478962468344199, 2.839277441736703], 8, -1), ([8.4789624683442, 2.839277441736703], 15, -1), ([9.344987872128637, 2.3392774417367024], 20, -1), ([9.171339694461707, 3.3240851947489105], 12, -1), ([8.305314290677268, 3.8240851947489105], 3, -1), ([7.32050653766506, 3.65043701708198], 8, -1), ([8.260199158450968, 3.3084168737563115], 31, -1), ([9.245006911463175, 3.482065051423242], 20, -1), ([9.245006911463175, 2.482065051423242], 20, -1), ([8.602219301776636, 3.24810949454222], 18, -1), ([9.245006911463175, 2.482065051423242], 6, -1), ([8.305314290677266, 2.1400449080975736], 21, -1), ([9.171339694461704, 1.6400449080975732], 13, -1), ([8.528552084775164, 2.406089351216551], 28, -1), ([7.762507641656186, 1.7633017415300118], 25, -1), ([6.822815020870278, 1.4212815982043432], 6, -1), ([6.0567705777513, 0.7784939885178039], 18, -1), ([5.0567705777513, 0.778493988517804], 5, -1), ([5.922795981535739, 1.278493988517804], 28, -1), ([5.922795981535739, 2.2784939885178037], 21, -1), ([5.280008371849199, 1.5124495453988258], 26, -1), ([4.937988228523531, 2.4521421661847342], 26, -1), ([4.937988228523531, 3.4521421661847342], 10, -1), ([5.922795981535739, 3.6257903438516648], 29, -1), ([6.907603734547947, 3.7994385215185953], 7, -1), ([5.907603734547947, 3.7994385215185953], 35, -50), ([5.0415783307635085, 4.299438521518595], 28, -1), ([5.981270951549417, 3.9574183781929264], 34, -50), ([6.747315394668395, 4.600205987879466], 3, -50), ([7.732123147680603, 4.773854165546396], 3, -1), ([8.498167590799582, 5.416641775232936], 33, -1), ([8.156147447473913, 4.476949154447027], 19, -50), ([7.216454826688005, 4.818969297772696], 28, -1), ([6.3504294229035665, 5.318969297772696], 9, -1), ([5.8504294229035665, 6.184994701557135], 19, -1), ([5.8504294229035665, 7.184994701557135], 3, -1), ([5.3504294229035665, 8.051020105341573], 20, -1), ([5.993217032590106, 7.284975662222595], 27, -1), ([6.759261475709084, 6.6421880525360555], 25, -1), ([6.759261475709084, 7.6421880525360555], 19, -1), ([7.259261475709084, 6.776162648751617], 34, -1), ([7.259261475709084, 5.776162648751617], 17, -1), ([7.259261475709084, 6.776162648751617], 27, -1), ([8.125286879493522, 6.276162648751617], 12, -1), ([7.783266736167854, 5.336470027965708], 30, -50), ([6.917241332383416, 4.836470027965708], 29, -1), ([7.902049085395624, 5.010118205632638], 14, -1), ([8.768074489180062, 5.510118205632638], 27, -1), ([8.594426311513132, 6.4949259586448465], 22, -1), ([9.237213921199672, 5.7288815155258686], 6, -1), ([8.594426311513132, 4.962837072406891], 0, -1), ([8.094426311513132, 5.828862476191329], 29, -1), ([8.436446454838801, 4.889169855405421], 1, -1), ([8.936446454838801, 5.755195259189859], 32, -1), ([9.936446454838801, 5.755195259189859], 28, -1), ([8.996753834052893, 6.097215402515529], 27, -1), ([9.862779237837332, 6.597215402515529], 24, -1), ([9.862779237837332, 5.597215402515529], 27, -1), ([9.219991628150792, 4.831170959396551], 5, -1), ([9.219991628150792, 5.831170959396551], 30, -1), ([9.562011771476461, 4.891478338610643], 22, -1), ([9.219991628150792, 5.831170959396552], 24, -1), ([8.719991628150792, 4.965145555612113], 13, -1), ([8.893639805817722, 3.980337802599905], 28, -1), ([9.759665209602161, 3.4803378025999048], 26, -1), ([10.0, 4.420030423385813], 29, -1), ([10.0, 3.7772428136992735], 24, -1), ([10.0, 4.2772428136992735], 2, -1), ([10.0, 3.411217409914835], 24, -1), ([10.0, 3.753237553240504], 3, -1), ([10.0, 3.1104499435539643], 25, -1), ([9.35721239031346, 3.876494386672942], 30, -1), ([8.71442478062692, 4.642538829791921], 9, -1), ([9.699232533639128, 4.816187007458851], 28, -1), ([9.357212390313459, 3.876494386672942], 29, -1), ([10.0, 3.3764943866729418], 12, -1), ([10.0, 4.24251979045738], 9, -1), ([10.0, 3.9004996471317117], 14, -1), ([10.0, 2.9608070263458033], 7, -1), ([10.0, 3.9456147793580114], 34, -1), ([10.0, 3.9456147793580114], 24, -1), ([9.82635182233307, 4.930422532370219], 21, -1), ([9.18356421264653, 5.696466975489197], 11, -1), ([9.82635182233307, 6.462511418608175], 32, -1), ([10.0, 6.1204912752825065], 35, -1), ([9.13397459621556, 5.6204912752825065], 35, -1), ([10.0, 6.1204912752825065], 23, -1), ([10.0, 6.986516679066945], 23, -1), ([9.82635182233307, 7.971324432079153], 23, -1), ([10.0, 7.031631811293245], 7, -1), ([9.060307379214091, 6.689611667967577], 26, -1), ([9.233955556881021, 7.674419420979785], 7, -1), ([9.407603734547951, 6.689611667967577], 7, -1), ([9.74962387787362, 7.6293042887534845], 25, -1), ([8.764816124861413, 7.802952466420415], 18, -1), ([8.422795981535744, 8.742645087206323], 18, -1), ([8.422795981535744, 9.742645087206323], 1, -1), ([9.065583591222284, 10.0], 35, -1), ([8.199558187437844, 9.5], 22, -1), ([9.065583591222284, 10.0], 26, -1), ([9.065583591222284, 10.0], 15, -1), ([10.0, 9.65797985667433], 35, -1), ([10.0, 10.0], 5, -1), ([9.65797985667433, 9.060307379214091], 12, -1), ([10.0, 8.194281975429652], 16, -1), ([10.0, 7.254589354643744], 9, -1), ([9.13397459621556, 6.754589354643744], 33, -1), ([8.149166843203353, 6.928237532310674], 13, -1), ([8.791954452889893, 7.694281975429652], 1, -1), ([8.618306275222963, 8.679089728441859], 24, -1), ([7.633498522210755, 8.505441550774929], 22, -1), ([8.276286131897294, 9.271485993893908], 10, -1), ([7.776286131897294, 10.0], 2, -1), ([7.434265988571625, 10.0], 31, -1), ([6.494573367785717, 9.65797985667433], 35, -1), ([6.320925190118787, 8.673172103662123], 9, -1), ([7.260617810904696, 9.015192246987793], 22, -1), ([8.260617810904696, 9.015192246987793], 4, -1), ([9.245425563916903, 8.841544069320863], 12, -1), ([8.305732943130995, 8.499523925995193], 7, -1), ([7.320925190118786, 8.673172103662123], 31, -1), ([6.820925190118786, 9.539197507446563], 25, -1), ([6.820925190118786, 8.539197507446563], 28, -1), ([7.8057329431309945, 8.712845685113493], 30, -1), ([8.448520552817534, 9.478890128232472], 10, -1), ([8.790540696143204, 10.0], 2, -1), ([7.850848075357295, 9.65797985667433], 4, -1), ([6.911155454571387, 10.0], 29, -1), ([7.911155454571387, 10.0], 32, -1), ([7.268367844884847, 9.233955556881021], 2, -1), ([6.268367844884847, 9.233955556881021], 20, -1), ([7.253175597897055, 9.060307379214091], 23, -1), ([6.753175597897055, 9.92633278299853], 12, -1), ([7.692868218682964, 9.584312639672861], 22, -1), ([8.335655828369504, 8.818268196553882], 21, -1), ([8.335655828369504, 9.818268196553882], 5, -1), ([7.395963207583595, 9.476248053228213], 18, -1), ([7.053943064257926, 8.536555432442304], 34, -1), ([6.069135311245718, 8.710203610109234], 5, -1), ([7.069135311245718, 8.710203610109234], 20, -1), ([6.069135311245718, 8.710203610109234], 7, -1), ([6.411155454571388, 7.770510989323326], 3, -1), ([6.069135311245719, 6.830818368537418], 35, -1), ([6.711922920932259, 7.596862811656396], 11, -1), ([6.711922920932259, 8.596862811656397], 5, 100)]\n"
          ]
        }
      ],
      "source": [
        "#@title Play a sample episode\n",
        "\n",
        "env = World()\n",
        "\n",
        "episode = []\n",
        "\n",
        "state = env.reset()\n",
        "done = False\n",
        "\n",
        "while not done:\n",
        "\n",
        "    action = np.random.choice(36)  # randomly choose an action\n",
        "    reward, next_state, done = env.transition(deepcopy(state), action)\n",
        "    episode.append(deepcopy((state, action, reward)))\n",
        "    state = next_state\n",
        "\n",
        "print(episode)"
      ],
      "id": "81e2038c"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bl-RpgULHPff",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Modified agent, including training\n",
        "\n",
        "class TemporalDifference:\n",
        "    def __init__(self, Env, alpha=0.1, gamma=0.9, epsilon=0.1, lambd=0.9):\n",
        "        self.Env = Env\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "        self.epsilon = epsilon\n",
        "        self.lambd = lambd\n",
        "\n",
        "        self.state_dim = self.Env._get_state_dim()\n",
        "        self.action_dim = self.Env._get_action_dim()[0]\n",
        "\n",
        "        # Initialize the neural network for Q approximation\n",
        "        self.model = self._build_model()\n",
        "\n",
        "    def _build_model(self):\n",
        "        inputs = Input(shape=(2,))  # Two inputs for x and y coordinates\n",
        "        x = Dense(24, activation='relu')(inputs)\n",
        "        x = Dense(24, activation='relu')(x)\n",
        "        x = Dense(24, activation='relu')(x)\n",
        "        outputs = Dense(self.action_dim, activation='linear')(x)  # One output per action\n",
        "        model = Model(inputs=inputs, outputs=outputs)\n",
        "        model.compile(optimizer=Adam(learning_rate=self.alpha), loss='mse')\n",
        "        return model\n",
        "\n",
        "    def epsilon_greedy_policy(self, state):\n",
        "        if np.random.rand() < self.epsilon:\n",
        "            return np.random.choice(self.action_dim)  # Explore\n",
        "        else:\n",
        "            q_values = self.model.predict(np.array([state]))[0]  # Exploit\n",
        "            return np.argmax(q_values)\n",
        "\n",
        "    def train(self, num_episodes, on_policy=True):\n",
        "        start_time = time.time()  # Record the start time\n",
        "        rewards_per_episode = []  # For visualization\n",
        "\n",
        "        for _ in tqdm(range(num_episodes)):\n",
        "            episode_reward = 0  # Initialize reward for the episode\n",
        "            state = self.Env.reset()\n",
        "            #done = False\n",
        "            action = self.epsilon_greedy_policy(state)\n",
        "\n",
        "            while not self.Env.is_done:\n",
        "                #action = self.epsilon_greedy_policy(state)\n",
        "                reward, next_state, done = self.Env.transition(state, action)\n",
        "                episode_reward += reward\n",
        "\n",
        "                target = reward\n",
        "                if not self.Env.is_done:\n",
        "                    next_q_values = self.model.predict(np.array([next_state]))[0]\n",
        "                    if on_policy:\n",
        "                        next_action = self.epsilon_greedy_policy(next_state)\n",
        "                        target += self.gamma * next_q_values[next_action]\n",
        "                    else:\n",
        "                        target += self.gamma * np.max(next_q_values)\n",
        "\n",
        "                target_vec = self.model.predict(np.array([state]))[0]\n",
        "                target_vec[action] = target\n",
        "\n",
        "                self.model.fit(np.array([state]), np.array([target_vec]), epochs=1, verbose=0)\n",
        "\n",
        "                state = next_state\n",
        "\n",
        "            rewards_per_episode.append(episode_reward)\n",
        "\n",
        "        end_time = time.time()  # Record the end time\n",
        "        total_time = end_time - start_time  # Calculate the total training time\n",
        "\n",
        "        return rewards_per_episode, total_time"
      ],
      "id": "Bl-RpgULHPff"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ItcnMF5ocqe4",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Functions for visualization\n",
        "\n",
        "def plot_value_function(agent, env, resolution=20):\n",
        "    x = np.linspace(0, env.max_row, resolution)\n",
        "    y = np.linspace(0, env.max_col, resolution)\n",
        "    X, Y = np.meshgrid(x, y)\n",
        "    Z = np.zeros_like(X)\n",
        "\n",
        "    for i in range(resolution):\n",
        "        for j in range(resolution):\n",
        "            state = np.array([X[i, j], Y[i, j]])\n",
        "            q_values = agent.model.predict(state.reshape(1, -1))[0]\n",
        "            Z[i, j] = np.max(q_values)  # For the value function\n",
        "\n",
        "    plt.contourf(X, Y, Z, levels=50, cmap='viridis')\n",
        "    plt.colorbar()\n",
        "    plt.xlabel('X Position')\n",
        "    plt.ylabel('Y Position')\n",
        "    plt.title('Value Function')\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "def plot_policy(agent, env, resolution = 20):\n",
        "    '''\n",
        "    For each point in a discretized version of the state space,\n",
        "    we can plot the action that the policy would choose.\n",
        "    This can be represented using arrows showing the direction the agent would take from different points.\n",
        "    '''\n",
        "    x = np.linspace(0, env.max_row, resolution)\n",
        "    y = np.linspace(0, env.max_col, resolution)\n",
        "    X, Y = np.meshgrid(x, y)\n",
        "\n",
        "    U, V = np.zeros_like(X), np.zeros_like(Y)\n",
        "\n",
        "    for i in range(resolution):\n",
        "        for j in range(resolution):\n",
        "            state = np.array([X[i, j], Y[i, j]])\n",
        "            action = agent.epsilon_greedy_policy(state)\n",
        "            # Convert the action into a change in x and y (U, V)\n",
        "            angle = action * 10  # Assuming each action corresponds to a 10-degree increment\n",
        "            U[i, j] = np.cos(np.radians(angle))\n",
        "            V[i, j] = np.sin(np.radians(angle))\n",
        "\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.quiver(X, Y, U, V, pivot='mid')\n",
        "    plt.xlabel('X Position')\n",
        "    plt.ylabel('Y Position')\n",
        "    plt.title('Policy Visualization')\n",
        "    plt.grid(True)\n",
        "    plt.show()"
      ],
      "id": "ItcnMF5ocqe4"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V17dJ_SIIux8",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Agent 1\n",
        "\n",
        "env = World()\n",
        "n = 50\n",
        "\n",
        "# Case 1: Monte Carlo\n",
        "agent1 = TemporalDifference(\n",
        "    env, alpha=0.1, gamma=0.9, epsilon=0.1, lambd=1\n",
        ")\n",
        "reward1, total_time1 = agent1.train(num_episodes=n)\n",
        "\n",
        "# After training your agent:\n",
        "# plot_value_function(agent1, env)\n",
        "\n",
        "plt.plot(reward1)\n",
        "plt.xlabel('Episode')\n",
        "plt.ylabel('Cumulative Reward')\n",
        "plt.title('Agent1 Learning Over Time')\n",
        "plt.show()"
      ],
      "id": "V17dJ_SIIux8"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xvAhosyhdE_U"
      },
      "outputs": [],
      "source": [
        "plot_policy(agent1, env)"
      ],
      "id": "xvAhosyhdE_U"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lYMicltbe972",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Agent 2\n",
        "\n",
        "env = World()\n",
        "n = 50\n",
        "\n",
        "## Case 2: SARSA\n",
        "agent2 = TemporalDifference(\n",
        "    env, alpha=0.1, gamma=0.9, epsilon=0.1, lambd=0\n",
        ")\n",
        "reward2, total_time2 = agent2.train(num_episodes=n)\n",
        "\n",
        "# plot_value_function(agent2, env)\n",
        "\n",
        "plt.plot(reward2)\n",
        "plt.xlabel('Episode')\n",
        "plt.ylabel('Cumulative Reward')\n",
        "plt.title('Agent2 Learning Over Time')\n",
        "plt.show()"
      ],
      "id": "lYMicltbe972"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8N1whb8he11t"
      },
      "outputs": [],
      "source": [
        "plot_policy(agent2, env)"
      ],
      "id": "8N1whb8he11t"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1K4BFIaUvhYF",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Agent 3\n",
        "\n",
        "env = World()\n",
        "n = 50\n",
        "\n",
        "## Case 3: Q-Learning\n",
        "agent3 = TemporalDifference(\n",
        "    env, alpha=0.1, gamma=0.9, epsilon=0.1, lambd=0\n",
        ")\n",
        "reward3, total_time3 = agent3.train(num_episodes=n, on_policy=False)\n",
        "\n",
        "plt.plot(reward3)\n",
        "plt.xlabel('Episode')\n",
        "plt.ylabel('Cumulative Reward')\n",
        "plt.title('Agent3 Learning Over Time')\n",
        "plt.show()"
      ],
      "id": "1K4BFIaUvhYF"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OYGa7zAMvzKC"
      },
      "outputs": [],
      "source": [
        "plot_policy(agent3, env)"
      ],
      "id": "OYGa7zAMvzKC"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "c39g5M6pSCh_",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Agent 4\n",
        "\n",
        "env = World()\n",
        "n = 50\n",
        "\n",
        "## Case 4: General TD(λ)\n",
        "agent4 = TemporalDifference(\n",
        "    env, alpha=0.1, gamma=0.9, epsilon=0.1, lambd=0.5\n",
        ")\n",
        "reward4, total_time4 = agent4.train(num_episodes=n)\n",
        "\n",
        "# plot_value_function(agent4, env)\n",
        "\n",
        "plt.plot(reward4)\n",
        "plt.xlabel('Episode')\n",
        "plt.ylabel('Cumulative Reward')\n",
        "plt.title('Agent4 Learning Over Time')\n",
        "plt.show()"
      ],
      "id": "c39g5M6pSCh_"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0ZItfDcVSa1y"
      },
      "outputs": [],
      "source": [
        "plot_policy(agent4, env)"
      ],
      "id": "0ZItfDcVSa1y"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_a0UdjNMVynl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1f724abb-0191-4d14-e0b7-6d1a883a8dbb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4789.866044521332 5033.943603038788\n"
          ]
        }
      ],
      "source": [
        "print(total_time1, total_time4)"
      ],
      "id": "_a0UdjNMVynl"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4nDm6jrh-Frn",
        "outputId": "04e90cc2-b3a9-4e10-887e-3aa840ffd6cc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "4144.591289520264 3684.470736026764\n"
          ]
        }
      ],
      "source": [
        "print(total_time2, total_time3)"
      ],
      "id": "4nDm6jrh-Frn"
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}